import requests
from BeautifulSoup import BeautifulSoup
import re
from time import sleep

def crawl(url):
    while True:
	try:
	    r=requests.get(url)
	    return r.content
	    break
	except Exception as e:
	    print e
	    sleep(2)
	    print "Retrying!!"

base_url = 'http://www.squawka.com/teams/chelsea/squad'
print "Starting to crawl"
html = crawl(base_url)
print "Crawled page!"
soup=BeautifulSoup(html)
names=soup.findAll("td", { "class" : "squad_playerphoto" })
for name in names:
	
